var store = [{
        "title": "Hamiltonian Monte Carlo",
        "excerpt":"$\\renewcommand{\\hat}[1]{\\widehat{#1}}$ # %conda install tqdm==4.66.5 scipy==1.13.1 plotly==5.24.1 numpy==1.26.4 pandas==2.2.2 openblas==0.3.21 matplotlib==3.9.2 import numpy as np # import plotly.graph_objects as go # from plotly.subplots import make_subplots import matplotlib.pyplot as plt from tqdm import tqdm from scipy.stats import norm, multivariate_normal, uniform, beta from samplers import hmc, metro_hastings, R_hat, plot_chains, param_scatter n_iters =...","categories": ["MCMC"],
        "tags": [],
        "url": "/mcmc/Hamiltonian-MC-Intro/",
        "teaser": null
      },{
        "title": "Comparing Bandit Algorithms",
        "excerpt":"$\\renewcommand{\\hat}[1]{\\widehat{#1}}$ Multi Armed Bandit Simulations Multi-armed bandit algorithms optimise a target metric by learning which choice of $K$ actions (a.k.a. “arms”) to take. The value of the metric after taking an action is considered a reward for that action. In making a choice of action there is a tradeoff between...","categories": ["Bandits"],
        "tags": [],
        "url": "/bandits/Comparing-Bandit-Algorithms/",
        "teaser": null
      }]
