<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="https://c-ldwc.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://c-ldwc.github.io/" rel="alternate" type="text/html" /><updated>2024-11-30T18:58:44+11:00</updated><id>https://c-ldwc.github.io/feed.xml</id><title type="html">Charles Ludowici Portfolio</title><subtitle></subtitle><author><name>C Ludowici, PhD</name></author><entry><title type="html">Comparing Bandit Algorithms</title><link href="https://c-ldwc.github.io/bandits/Comparing-Bandit-Algorithms/" rel="alternate" type="text/html" title="Comparing Bandit Algorithms" /><published>2024-11-29T21:46:32+11:00</published><updated>2024-11-29T21:46:32+11:00</updated><id>https://c-ldwc.github.io/bandits/Comparing-Bandit-Algorithms</id><content type="html" xml:base="https://c-ldwc.github.io/bandits/Comparing-Bandit-Algorithms/"><![CDATA[<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
  </script>

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p>$\renewcommand{\hat}[1]{\widehat{#1}}$</p>
<h1 id="multi-armed-bandit-simulations">Multi Armed Bandit Simulations</h1>

<p>Multi-armed bandit algorithms optimise a target metric by learning which choice of $K$ actions (a.k.a. “arms”) to take. The value of the metric after taking an action is considered a reward for that action.  In making a choice of action there is a tradeoff between exploring each action in order to better understand its expected reward, or exploiting the action that has led to the best rewards in the past. Bandit algorithms are applied in marketing, finance, conversion rate optimisation and clinical trials.</p>

<p>In this notebook I compare sims from two popular bandit algorithms - Thompson sampling, which is based on the Bayesian update to a binomial mean from a beta prior, and the Upper Confidence Bound algorithm, which uses Hoeffding’s inequality to bound the mean. This post is designed to highlight my fluency with Python and these algorithms - rather than providing a serious scientific comparison of the two methods.</p>

<p>My bandit algorithms, reward and some utility functions are packaged in the <code class="language-plaintext highlighter-rouge">bandits</code> module imported above. The software lives in my <a href="https://github.com/c-ldwc/Portfolio/tree/main/Bandits/bandits">github portfolio</a>. The algorithms are found in the <a href="https://github.com/c-ldwc/Portfolio/blob/main/Bandits/bandits/banditClasses.py">bandit algorithm classes file</a>.</p>

<p>I simulated bandits with a time horizon $T$ of 10,000. Each bandit was simulated 500 times. Giving me 1,000 observations at each T for both UCB and Thompson sampling. The bandits sampled from 2 arms, both returned Bernoulli rewards with p = 0.2 and p = 0.3. We want to know how our algorithm performs, on average, relative to the best performance. This is known as regret and is defined as 
\(R = \Sigma_{i=1}^{T}\left[\mu(a^*) - \mu(a_t)\right]\)</p>

<p>where $\mu(a^*)$ is the mean for the best action and $\mu(a_t)$ is the mean for the action chosen at time $t$. For our rewards it is</p>

\[R = T*.3 - \Sigma_{i=1}^{T}\mu(a_t)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">bandits</span> <span class="c1">#bandit algos and rewards
</span><span class="kn">from</span> <span class="nn">simulation_help</span> <span class="kn">import</span> <span class="p">(</span> <span class="c1">#plotting and simulation functions
</span>    <span class="n">bandit_summaries</span><span class="p">,</span>
    <span class="n">simulate</span><span class="p">,</span>
    <span class="n">TS_single_sim</span><span class="p">,</span>
    <span class="n">ucb_single_sim</span><span class="p">,</span>
    <span class="n">mean_plots</span><span class="p">,</span>
    <span class="n">regret_plots</span><span class="p">,</span>
    <span class="n">bound_plots</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">arm_mu</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]</span>  <span class="c1"># True means for the binomial reward
</span><span class="n">T</span> <span class="o">=</span> <span class="mi">10_000</span>  <span class="c1"># Time horizon for each bandit
</span><span class="n">n_sims</span> <span class="o">=</span> <span class="mi">500</span>  <span class="c1"># Number of bandits to simulate
</span></code></pre></div></div>

<h2 id="simulations">Simulations</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># stationary simulations
</span><span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">exists</span><span class="p">(</span><span class="s">"simulations.csv"</span><span class="p">):</span>
    <span class="n">rg</span> <span class="o">=</span> <span class="n">bandits</span><span class="p">.</span><span class="n">binom_reward_generator</span><span class="p">(</span><span class="n">arm_mu</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">TS_data</span> <span class="o">=</span> <span class="n">simulate</span><span class="p">(</span>
        <span class="n">N</span><span class="o">=</span><span class="n">n_sims</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="n">T</span><span class="p">,</span> <span class="n">arm_mu</span><span class="o">=</span><span class="n">arm_mu</span><span class="p">,</span> <span class="n">single_sim</span><span class="o">=</span><span class="n">TS_single_sim</span><span class="p">,</span> <span class="n">reward_fn</span><span class="o">=</span><span class="n">rg</span><span class="p">.</span><span class="n">reward</span>
    <span class="p">)</span>
    <span class="n">ucb_data</span> <span class="o">=</span> <span class="n">simulate</span><span class="p">(</span>
        <span class="n">N</span><span class="o">=</span><span class="n">n_sims</span><span class="p">,</span>
        <span class="n">T</span><span class="o">=</span><span class="n">T</span><span class="p">,</span>
        <span class="n">n_out</span><span class="o">=</span><span class="n">T</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">arm_mu</span><span class="p">),</span>
        <span class="n">arm_mu</span><span class="o">=</span><span class="n">arm_mu</span><span class="p">,</span>
        <span class="n">single_sim</span><span class="o">=</span><span class="n">ucb_single_sim</span><span class="p">,</span>
        <span class="n">reward_fn</span><span class="o">=</span><span class="n">rg</span><span class="p">.</span><span class="n">reward</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">TS_data</span><span class="p">[</span><span class="s">"source"</span><span class="p">]</span> <span class="o">=</span> <span class="s">"Thompson Sampling"</span>
    <span class="n">ucb_data</span><span class="p">[</span><span class="s">"source"</span><span class="p">]</span> <span class="o">=</span> <span class="s">"UCB"</span>
    <span class="n">sims</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">TS_data</span><span class="p">,</span> <span class="n">ucb_data</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">sims</span><span class="p">[</span><span class="s">"act"</span><span class="p">]</span> <span class="o">=</span> <span class="n">sims</span><span class="p">[</span><span class="s">"act"</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">sims</span><span class="p">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s">"simulations.csv"</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">sims</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"simulations.csv"</span><span class="p">)</span>

<span class="n">stat_summaries</span> <span class="o">=</span> <span class="n">bandit_summaries</span><span class="p">(</span><span class="n">sims</span><span class="p">,</span> <span class="n">arm_mu</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="stationary-rewards">Stationary Rewards</h1>
<h2 id="mean-reward-for-each-action-over-time">Mean reward for each action over time</h2>

<p>Here we see how the mean rewards for each action over each iteration of the bandit algos evolve. Each blue line is a run of the bandit simulator. The x axis is the $t^{th}$ performance of the relevant action each run. At x = 1000 in the top left plot we see the mean reward for the first action for all Thompson sampling runs that chose that action 1000 or more times.</p>

<p>We can see that Thompson sampling tends to learn about the poorer action (k = 0) early on and rarely runs it more than about 400 times. The UCB algorithm however takes much longer for its intervals to converge to a point where this action is obviously not a winner.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mean_plots</span><span class="p">(</span><span class="n">stat_summaries</span><span class="p">[</span><span class="s">"running_means"</span><span class="p">]);</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 500/500 [00:03&lt;00:00, 137.38it/s]
100%|██████████| 500/500 [00:35&lt;00:00, 14.22it/s]
100%|██████████| 500/500 [00:07&lt;00:00, 71.21it/s]
100%|██████████| 500/500 [00:32&lt;00:00, 15.22it/s]
</code></pre></div></div>

<p><img src="/assets/images/2024-11-30%20Comparing%20Bandit%20Algorithms_7_1.png" alt="png" /></p>

<h2 id="regret">Regret</h2>
<p>The plots below show the cumulative regret at each point during a simulation run. Each line represents a bandit simulation. Thompson sampling appears initially linear (i.e. described by a power law because these are log-log plots) and levels off around t = 1000. UCB does not show this patter and attains a higher regret than almost all Thompson Sampling simulations.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">regret_plots</span><span class="p">(</span><span class="n">stat_summaries</span><span class="p">[</span><span class="s">"regret_trajectories"</span><span class="p">],</span> <span class="n">T</span><span class="p">);</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 499/499 [00:00&lt;00:00, 535.24it/s]
</code></pre></div></div>

<p><img src="/assets/images/2024-11-30%20Comparing%20Bandit%20Algorithms_9_1.png" alt="png" /></p>

<h2 id="parameter-bounds">Parameter Bounds</h2>

<p>Finally, we can view the bounds on the estimates of the mean for each algorithm. For Thompson sampling these are the 95% quantiles of the posterior distribution. For the UCB algorithm they are the confidence intervals derived from the Hoeffding Inequality (these can be $\lt 0$).  We can see that UCB’s bounds are wider than Thompson sampling, but this is slightly arbitrary because it depends on my choice of quantile for the Thompson posterior.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bound_plots</span><span class="p">(</span><span class="n">sims</span><span class="p">);</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 499/499 [00:02&lt;00:00, 217.35it/s]
100%|██████████| 499/499 [00:02&lt;00:00, 224.76it/s]
</code></pre></div></div>

<p><img src="/assets/images/2024-11-30%20Comparing%20Bandit%20Algorithms_11_1.png" alt="png" /></p>

<h1 id="non-stationary-rewards">Non-Stationary Rewards</h1>

<p>In addition to the above stationary rewards, we want to see what happens when the mean is changing over time. For e-commerce, this is pretty common. Conversion is more likely at certain times of the week (i.e. weekends), or year (i.e. sales events). To simulate this I have a reward generator that increases the Bernoulli parameter by .1 every 6th and 7th day - corresponding to a weekend bump in conversions. Note that this preserves the ranking of the actions.</p>

<p>I do this because these two algorithms are predicated on stationary reward distributions, but such distributions are very rare for almost all metrics we care about. Checking that the algorithms’ decisions are robust to deviations from their ideal cases is vital if we want to deploy these to automate decision making.</p>

<p>The plots below show very little in terms of interesting changes due to the non-stationarity - probably because the weekend bump preserves the order of the two arms’ means.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># non-stationary simulations
</span><span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">exists</span><span class="p">(</span><span class="s">"non_stat_simulations.csv"</span><span class="p">):</span>
    <span class="n">rg</span> <span class="o">=</span> <span class="n">bandits</span><span class="p">.</span><span class="n">binom_weekend_reward</span><span class="p">(</span><span class="n">arm_mu</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">TS_data</span> <span class="o">=</span> <span class="n">simulate</span><span class="p">(</span>
        <span class="n">N</span><span class="o">=</span><span class="n">n_sims</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="n">T</span><span class="p">,</span> <span class="n">arm_mu</span><span class="o">=</span><span class="n">arm_mu</span><span class="p">,</span> <span class="n">single_sim</span><span class="o">=</span><span class="n">TS_single_sim</span><span class="p">,</span> <span class="n">reward_fn</span><span class="o">=</span><span class="n">rg</span><span class="p">.</span><span class="n">reward</span>
    <span class="p">)</span>
    <span class="n">ucb_data</span> <span class="o">=</span> <span class="n">simulate</span><span class="p">(</span>
        <span class="n">N</span><span class="o">=</span><span class="n">n_sims</span><span class="p">,</span>
        <span class="n">T</span><span class="o">=</span><span class="n">T</span><span class="p">,</span>
        <span class="n">n_out</span><span class="o">=</span><span class="n">T</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">arm_mu</span><span class="p">),</span>
        <span class="n">arm_mu</span><span class="o">=</span><span class="n">arm_mu</span><span class="p">,</span>
        <span class="n">single_sim</span><span class="o">=</span><span class="n">ucb_single_sim</span><span class="p">,</span>
        <span class="n">reward_fn</span><span class="o">=</span><span class="n">rg</span><span class="p">.</span><span class="n">reward</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">TS_data</span><span class="p">[</span><span class="s">"source"</span><span class="p">]</span> <span class="o">=</span> <span class="s">"Thompson Sampling"</span>
    <span class="n">ucb_data</span><span class="p">[</span><span class="s">"source"</span><span class="p">]</span> <span class="o">=</span> <span class="s">"UCB"</span>
    <span class="n">ns_sim</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">TS_data</span><span class="p">,</span> <span class="n">ucb_data</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">ns_sim</span><span class="p">[</span><span class="s">"act"</span><span class="p">]</span> <span class="o">=</span> <span class="n">ns_sim</span><span class="p">[</span><span class="s">"act"</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">ns_sim</span><span class="p">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s">"non_stat_simulations.csv"</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">ns_sim</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"non_stat_simulations.csv"</span><span class="p">)</span>

<span class="n">ns_summaries</span> <span class="o">=</span> <span class="n">bandit_summaries</span><span class="p">(</span><span class="n">ns_sim</span><span class="p">,</span> <span class="n">arm_mu</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mean_plots</span><span class="p">(</span><span class="n">ns_summaries</span><span class="p">[</span><span class="s">"running_means"</span><span class="p">]);</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 500/500 [00:03&lt;00:00, 139.73it/s]
100%|██████████| 500/500 [00:35&lt;00:00, 13.89it/s]
100%|██████████| 500/500 [00:07&lt;00:00, 71.36it/s]
100%|██████████| 500/500 [00:33&lt;00:00, 15.09it/s]
</code></pre></div></div>

<p><img src="/assets/images/2024-11-30%20Comparing%20Bandit%20Algorithms_14_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">regret_plots</span><span class="p">(</span><span class="n">ns_summaries</span><span class="p">[</span><span class="s">"regret_trajectories"</span><span class="p">],</span> <span class="n">T</span><span class="p">);</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 499/499 [00:00&lt;00:00, 581.05it/s]
</code></pre></div></div>

<p><img src="/assets/images/2024-11-30%20Comparing%20Bandit%20Algorithms_15_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bound_plots</span><span class="p">(</span><span class="n">ns_sim</span><span class="p">);</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 499/499 [00:02&lt;00:00, 215.17it/s]
100%|██████████| 499/499 [00:02&lt;00:00, 223.29it/s]
</code></pre></div></div>

<p><img src="/assets/images/2024-11-30%20Comparing%20Bandit%20Algorithms_16_1.png" alt="png" /></p>]]></content><author><name>C Ludowici, PhD</name></author><category term="Bandits" /><summary type="html"><![CDATA[Implementing, Simulating and Comparing the Thompson Sampling and Upper Confidence Bound algorithms]]></summary></entry><entry><title type="html">Hamiltonian Monte Carlo</title><link href="https://c-ldwc.github.io/mcmc/Hamiltonian-MC-Intro/" rel="alternate" type="text/html" title="Hamiltonian Monte Carlo" /><published>2024-11-16T21:46:32+11:00</published><updated>2024-11-16T21:46:32+11:00</updated><id>https://c-ldwc.github.io/mcmc/Hamiltonian-MC-Intro</id><content type="html" xml:base="https://c-ldwc.github.io/mcmc/Hamiltonian-MC-Intro/"><![CDATA[<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
  </script>

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p>$\renewcommand{\hat}[1]{\widehat{#1}}$</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># %conda install tqdm==4.66.5 scipy==1.13.1 plotly==5.24.1 numpy==1.26.4 pandas==2.2.2 openblas==0.3.21 matplotlib==3.9.2
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="c1"># import plotly.graph_objects as go
# from plotly.subplots import make_subplots
</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span><span class="p">,</span> <span class="n">multivariate_normal</span><span class="p">,</span> <span class="n">uniform</span><span class="p">,</span> <span class="n">beta</span>

<span class="kn">from</span> <span class="nn">samplers</span> <span class="kn">import</span> <span class="n">hmc</span><span class="p">,</span> <span class="n">metro_hastings</span><span class="p">,</span> <span class="n">R_hat</span><span class="p">,</span> <span class="n">plot_chains</span><span class="p">,</span> <span class="n">param_scatter</span>

<span class="n">n_iters</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">warmup</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">p</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">n_param</span> <span class="o">=</span> <span class="n">p</span> <span class="o">+</span> <span class="mi">1</span>
</code></pre></div></div>

<p>Markov Chain Monte Carlo (MCMC) algorithms draw samples from target probability distributions. The resulting samples can be used to approximate integrals (i.e. expectations) over the distribution being targeted. They are the workhorse of Bayesian computation, where posteriors are often too complex to solve without algorithmic tools, and are responsible for computation in the popular Bayesian software packages Stan and PyMC.</p>

<p>I use PyMC fairly regularly, but my understanding of the MCMC algo it uses (a variant of HMC) was largely based on intuition. The analyses in the notebooks in this folder (<a href="https://github.com/c-ldwc/Portfolio/tree/main/MCMC"><code class="language-plaintext highlighter-rouge">Portfolio/MCMC/</code></a>) are part of a self learning exercise where I use an implementation of Hamiltonian Monte Carlo (HMC) that I built in order to better understand how this class of algorithms works.</p>

<p>The HMC algorithm is based on the exposition in <a href="https://stat.columbia.edu/~gelman/book/">Gelman et al.</a> chapters 10, 11, and 12. The MCMC functions live in <a href="https://github.com/c-ldwc/Portfolio/tree/main/MCMC/samplers/samplers.py"><code class="language-plaintext highlighter-rouge">samplers/samplers.py</code></a>. There are some MCMC diagnostics in <a href="https://github.com/c-ldwc/Portfolio/tree/main/MCMC/samplers/utils.py"><code class="language-plaintext highlighter-rouge">samplers/utils.py</code></a></p>

<p>The <code class="language-plaintext highlighter-rouge">hmc</code> function is a Hamiltonian Monte Carlo (HMC) sampler.  It requires</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">log_prob</code>: an unnormed log probability for the distribution of interest,</li>
  <li><code class="language-plaintext highlighter-rouge">data</code>: a dictionary of data and parameters to pass to that log_prob. This is evaluated as part of each iteration of the algorithm</li>
  <li><code class="language-plaintext highlighter-rouge">grad</code>: the gradient of the distribution with regard to the parameters and a starting point for the parameter samples.</li>
  <li><code class="language-plaintext highlighter-rouge">n_iters</code>: the number of iterations for the algorithm</li>
  <li><code class="language-plaintext highlighter-rouge">starting</code>: a starting point for the samples</li>
  <li><code class="language-plaintext highlighter-rouge">eps</code>, <code class="language-plaintext highlighter-rouge">L</code>, <code class="language-plaintext highlighter-rouge">M</code>: Tuning parameters for the algorithms Hamiltonian dynamics. See <a href="https://stat.columbia.edu/~gelman/book/">Gelman et al.</a> chapter 12.</li>
</ul>

<p>The algorithm generates a “momentum” variable at the start of each iteration. It uses this and the gradient to explore the target log density through a discrete approximation to hamiltonian dynamics in physics. At the end of an iteration, it computes a ratio $r$ of the target density at the starting values and the final location of the iteration. It accepts the new location as a sample with probability $\min(r,1)$ - this is the same as the Metropolis Hastings algorithm’s acceptance step. Taking a ratio (actually a difference because we work with logs) means that normalisation constants cancel and we can work with unnormalised log densities as our target function.</p>

<p>At the end of the iterations, we have a “chain” of samples. A properly-tuned HMC run will converge to the target density, but that convergence takes time, so we drop a fixed number of starting iterations as a “warmup”. Convergence to the target density can be measured by visual and diagnostic tests that I discuss later.</p>

<h2 id="sampling-from-a-beta-distribution">Sampling From a Beta Distribution</h2>

<p>To see how it works, imagine we want to use it to draw samples from a Beta(x|a = 3,b = 5) distribution. We would never use MCMC for this in practice, because simpler methods like the ratio of gamma variables exist for Beta sampling, but it is a nice example of how the method works.</p>

<p>The algorithm works best if its sample space is unbounded so the search doesn’t reach areas of zero density. I use the logistic function to achieve this by mapping the real line to the support of the Beta distribution. The details of the derivation are in a footnote <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">inv_logit</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">log_prob_beta</span><span class="p">(</span><span class="n">proposal</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">inv_logit</span><span class="p">(</span><span class="n">proposal</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="o">+</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">proposal</span>
        <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">proposal</span><span class="p">))</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">grad_beta</span><span class="p">(</span><span class="n">proposal</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">inv_lgt_grad</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">proposal</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">proposal</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">inv_logit</span><span class="p">(</span><span class="n">proposal</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="p">((</span><span class="n">a</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">x</span> <span class="o">-</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">))</span> <span class="o">*</span> <span class="n">inv_lgt_grad</span>
        <span class="o">-</span> <span class="mi">1</span>
        <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">proposal</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">proposal</span><span class="p">))</span>
    <span class="p">)</span>


<span class="n">samples</span> <span class="o">=</span> <span class="n">hmc</span><span class="p">(</span>
    <span class="n">M</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="p">{</span><span class="s">"a"</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="s">"b"</span><span class="p">:</span> <span class="mi">3</span><span class="p">},</span>
    <span class="n">grad</span><span class="o">=</span><span class="n">grad_beta</span><span class="p">,</span>
    <span class="n">n_iter</span><span class="o">=</span><span class="n">n_iters</span><span class="p">,</span>
    <span class="n">log_prob</span><span class="o">=</span><span class="n">log_prob_beta</span><span class="p">,</span>
    <span class="n">starting</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">L</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>accept rate: 1.0: 100%|████████████████████| 1999/1999 [00:09&lt;00:00, 219.29it/s]
</code></pre></div></div>

<p>This results in the following chain of samples. The top plot is the untransformed variable. The lower plot is the inverse logit transformed variable. We can see that it’s mapped to $[0,1]$ and concentrating around the top part of the interval</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">warmup</span><span class="p">,</span> <span class="n">n_iters</span><span class="p">),</span> <span class="n">samples</span><span class="p">[</span><span class="n">warmup</span><span class="p">:])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Samples"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">warmup</span><span class="p">,</span> <span class="n">n_iters</span><span class="p">),</span> <span class="n">inv_logit</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="n">warmup</span><span class="p">:]))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Transformed Samples"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">();</span>
</code></pre></div></div>

<p><img src="/assets/images/output_6_0.png" alt="png" /></p>

<p>The sample mean is close to the true mean of 0.625. The quantiles and histogram of the samples are also in accordance with the expected shape.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"sample mean </span><span class="si">{</span><span class="n">inv_logit</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="n">warmup</span><span class="si">:</span><span class="p">]).</span><span class="n">mean</span><span class="p">()</span><span class="si">}</span><span class="s">, true mean </span><span class="si">{</span><span class="mi">5</span><span class="o">/</span><span class="mi">8</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s">"true .25, .5 and .75 quantiles </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">((</span><span class="n">beta</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">).</span><span class="n">ppf</span><span class="p">(.</span><span class="mi">25</span><span class="p">),</span><span class="n">beta</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">).</span><span class="n">ppf</span><span class="p">(.</span><span class="mi">5</span><span class="p">),</span> <span class="n">beta</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">).</span><span class="n">ppf</span><span class="p">(.</span><span class="mi">75</span><span class="p">)),</span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="s">"</span>
<span class="p">)</span>
<span class="k">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s">"sample .25, .5 and .75 quantiles </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">inv_logit</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="n">warmup</span><span class="si">:</span><span class="p">]),</span> <span class="p">(.</span><span class="mi">25</span><span class="p">,</span> <span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="p">.</span><span class="mi">75</span><span class="p">)),</span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="s">"</span>
<span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sample mean 0.6197045404423477, true mean 0.625
true .25, .5 and .75 quantiles [0.514 0.636 0.747]
sample .25, .5 and .75 quantiles [0.512 0.632 0.742]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s">"blue"</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">hist</span><span class="p">(</span><span class="n">inv_logit</span><span class="p">(</span><span class="n">samples</span><span class="p">),</span> <span class="mi">50</span><span class="p">)</span>

<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">();</span>
</code></pre></div></div>

<p><img src="/assets/images/output_9_0.png" alt="png" /></p>

<h2 id="linear-regression">Linear Regression</h2>

<p>This is a model for a linear regression with gaussian errors, an inverse gamma prior $\mathrm{inv_\Gamma}(1.5, 1)$ on the variance of the errors and independent normal priors on the coefficients of the model.</p>

<p>Because we want to constrain $\sigma^2$ to be positive. We work with $\sigma^2 = \exp(k)$ so we can let the sampler run over the real line for this parameter<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. Working with $\exp(k)$ requires a reparametisation of the prior density for $\sigma^2$</p>

<p>The prior on the variance is now</p>

\[p(\sigma^2) = p(\exp(k)) \\ \propto \exp(-k)^{2.5} \exp\left({-\exp(-k)}\right)\left\lvert\frac{d\sigma^2}{dk}\right\rvert \\ = \exp(-1.5k)\exp\left({-\exp(-k)}\right)\]

<p>Using $X_{i\cdot}$ as the $i^{th}$ row of X, the matrix of predictors, and $\mathbf{y}$ as the vector of outcome observations. The posterior density is
\(p(\theta|\mathbf{y}) \propto \prod_{i=1}^{N}\left[\frac{1}{\exp(k/2)} \exp \left(\frac{-(y_i - X_{i\cdot}\beta)^2}{2\exp(k)}\right)\right]\exp(-1.5k)\exp\left({-\exp(-k)}\right) \exp\left(\frac{-\beta^T\beta}{2}\right)\)</p>

<p>Ignoring the normalisation constant, the log posterior is</p>

\[\frac{-Nk}{2} - \sum_{i=1}^{N}\frac{(y_i - X_{i\cdot}\beta)^2}{2exp(k)} - 1.5k -\exp(-k) - \frac{\beta^T\beta}{2}\]

<p>The gradient for the coefficients is thus</p>

\[\sum_{i=1}^{N}\left[\frac{\left(y_i - X_{i\cdot}\beta\right)}{\exp(k)} X_{i\cdot}^T\right] - \beta\]

<p>The partial derivative for $k$ is</p>

\[-N/2 + \exp(-k)\sum_{i=1}^{N}\frac{(y_i - X_{i\cdot}\beta)^2}{2} -1.5  + \exp(-k)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">proposal</span><span class="p">):</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">proposal</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">proposal</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># variance
</span>    <span class="n">var</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>

    <span class="n">log_lik</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">var</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="p">((</span><span class="n">data</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span> <span class="o">-</span> <span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">k</span> <span class="o">/</span> <span class="mi">2</span>

    <span class="n">prior_coef</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">proposal</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">proposal</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">prior_sigma</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.5</span> <span class="o">*</span> <span class="n">k</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">var</span>

    <span class="k">return</span> <span class="n">log_lik</span> <span class="o">+</span> <span class="n">prior_coef</span> <span class="o">+</span> <span class="n">prior_sigma</span>


<span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">proposal</span><span class="p">):</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">proposal</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">proposal</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">var</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">coef_grad</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">data</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">/</span> <span class="n">var</span> <span class="o">*</span> <span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">proposal</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">sigma_grad</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.5</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">var</span> <span class="o">-</span> <span class="n">N</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">var</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">data</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)]</span>
    <span class="c1"># print(sigma_grad)
</span>    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">r_</span><span class="p">[</span><span class="n">coef_grad</span><span class="p">,</span> <span class="n">sigma_grad</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">create_regression</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">var</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">coef</span> <span class="o">=</span> <span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">).</span><span class="n">rvs</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

    <span class="n">X</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">p</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">p</span><span class="p">)).</span><span class="n">rvs</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>

    <span class="n">y</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">coef</span> <span class="o">+</span> <span class="n">norm</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span><span class="p">)).</span><span class="n">rvs</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">coef</span> <span class="o">=</span> <span class="n">coef</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">coef</span><span class="p">,</span> <span class="n">var</span>


<span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">coef</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">create_regression</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">var</span><span class="o">=</span><span class="mf">10.4</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"coefficients = </span><span class="si">{</span><span class="n">coef</span><span class="p">.</span><span class="n">flatten</span><span class="p">()</span><span class="si">}</span><span class="s">, variance = </span><span class="si">{</span><span class="n">var</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>coefficients = [ 1.85542175 -1.5029411   0.39253026], variance = 10.4
</code></pre></div></div>

<h3 id="sampling-the-posterior-and-checking-convergence">Sampling the posterior and checking convergence</h3>

<p>To check that our setup converges to the desired distribution, we run the sampler 4 times. Each sampler “chain” starts from different - ideally well separated - starting points. This is tricky to set up because if our starting points are too diffuse, the algorithm will take a long time to converge. If our starting points are all together, we run the risk of getting stuck in a local maximum. To check that we have converged, we want our runs to end up in the same region of posterior space. We can check this by visual inspection and the calculation of the $\hat{R}$ value, which is a ratio of between chain and within chain variance that will converge to 1 if the chains have mixed in the same region of posterior space.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">chains</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_iters</span><span class="p">,</span> <span class="n">coef</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>

<span class="k">for</span> <span class="n">chain</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">chains</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">hmc</span><span class="p">(</span>
        <span class="n">M</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n_param</span><span class="p">,</span> <span class="n">n_param</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.2</span><span class="p">,</span>
        <span class="n">data</span><span class="o">=</span><span class="p">{</span><span class="s">"data"</span><span class="p">:</span> <span class="n">y</span><span class="p">,</span> <span class="s">"X"</span><span class="p">:</span> <span class="n">X</span><span class="p">},</span>
        <span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">,</span>
        <span class="n">n_iter</span><span class="o">=</span><span class="n">n_iters</span><span class="p">,</span>
        <span class="n">log_prob</span><span class="o">=</span><span class="n">log_prob</span><span class="p">,</span>
        <span class="n">starting</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">r_</span><span class="p">[</span>
            <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">size</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">norm</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">).</span><span class="n">rvs</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span>
        <span class="p">],</span>
        <span class="n">eps</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
        <span class="n">L</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">chains</span><span class="p">[</span><span class="n">chain</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">params</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>accept rate: 1.0: 100%|████████████████████| 1999/1999 [00:17&lt;00:00, 114.15it/s]
accept rate: 1.0: 100%|████████████████████| 1999/1999 [00:17&lt;00:00, 114.21it/s]
accept rate: 1.0: 100%|█████████████████████| 1999/1999 [00:20&lt;00:00, 99.90it/s]
accept rate: 1.0: 100%|████████████████████| 1999/1999 [00:18&lt;00:00, 108.76it/s]
</code></pre></div></div>

<p>The $\hat{R}$ diagnostic value (the ratio of between and within chain variance) is less than 1.1 for all variables, and the chains appear to cover the same area in the plots, indicating that we have converged to the target distribution.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">R_hat</span><span class="p">(</span><span class="n">chains</span><span class="p">,</span> <span class="n">warmup</span><span class="p">).</span><span class="nb">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([1., 1., 1., 1.])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot_chains</span><span class="p">(</span><span class="n">chains</span><span class="p">,</span> <span class="n">warmup</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="s">"coef1"</span><span class="p">,</span> <span class="s">"coef2"</span><span class="p">,</span> <span class="s">"coef3"</span><span class="p">,</span> <span class="s">"sigma"</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/images/output_17_0.png" alt="png" /></p>

<p>Another way to check for issues with our chains is to plot scatterplots of pairs of our variables. There are no obvious degeneracies in the plots below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">samples</span> <span class="o">=</span> <span class="n">chains</span><span class="p">[:,</span> <span class="n">warmup</span><span class="p">:,</span> <span class="p">:].</span><span class="n">reshape</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_iters</span> <span class="o">-</span> <span class="n">warmup</span><span class="p">),</span> <span class="n">n_param</span><span class="p">)</span>

<span class="n">param_scatter</span><span class="p">(</span>
    <span class="n">samples</span><span class="p">,</span>
    <span class="n">warmup</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="s">"coef1"</span><span class="p">,</span> <span class="s">"coef2"</span><span class="p">,</span> <span class="s">"coef3"</span><span class="p">,</span> <span class="s">"sigma"</span><span class="p">],</span>
    <span class="n">plot_params</span><span class="o">=</span><span class="p">{</span><span class="s">"alpha"</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span> <span class="s">"s"</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s">"c"</span><span class="p">:</span> <span class="s">"k"</span><span class="p">},</span>
<span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/images/output_19_0.png" alt="png" /></p>

<h2 id="computing-things-we-care-about">Computing things we care about</h2>
<p>Using these samples we can estimate the expection of whatever function of the samples that we like. First, we need to drop the warmup samples and eliminate the chain dimension from our chains variable. We did this above when creating <code class="language-plaintext highlighter-rouge">params</code>.</p>

<p>Firstly, we can see that the sample means are close to the true values.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s">"Sample means for each parameter </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="n">r_</span><span class="p">[</span><span class="n">samples</span><span class="p">[</span><span class="si">:</span><span class="p">,</span><span class="si">:</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="si">:</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]).</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)]</span><span class="si">}</span><span class="s">"</span>
<span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"True values for each parameter </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="n">r_</span><span class="p">[</span><span class="n">coef</span><span class="p">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">var</span><span class="p">]</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Sample means for each parameter [ 1.72353675 -1.54830328  0.39778799 10.75540068]
True values for each parameter [ 1.85542175 -1.5029411   0.39253026 10.4       ]
</code></pre></div></div>

<p>We can also compute more interesting summaries. Firstly, the 90% quantiles of the marginal distributions.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="p">(</span><span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.975</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nb">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s">"coef1"</span><span class="p">,</span> <span class="s">"coef2"</span><span class="p">,</span> <span class="s">"coef3"</span><span class="p">,</span> <span class="s">"sigma"</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span>
            <span class="sa">f</span><span class="s">"There is a 90% probability that </span><span class="si">{</span><span class="n">names</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s"> has a value between </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">q</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">])</span><span class="si">}</span><span class="s"> and </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">q</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="n">i</span><span class="p">])</span><span class="si">}</span><span class="s">"</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span>
            <span class="sa">f</span><span class="s">"There is a 90% probability that </span><span class="si">{</span><span class="n">names</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s"> has a value between </span><span class="si">{</span><span class="n">q</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s"> and </span><span class="si">{</span><span class="n">q</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">"</span>
        <span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>There is a 90% probability that coef1 has a value between 1.512 and 1.939
There is a 90% probability that coef2 has a value between -1.749 and -1.347
There is a 90% probability that coef3 has a value between 0.198 and 0.597
There is a 90% probability that sigma has a value between 9.83551682472252 and 11.763482151980366
</code></pre></div></div>

<p>If we want to know the probability that the second variable is more important than the other two variables for our outcome, then this is as simple as taking a mean over an indicator of the relationship we are interested in.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">p_indicator</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span>
    <span class="p">(</span><span class="n">samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">samples</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">])</span>
<span class="p">).</span><span class="nb">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s">"The probability that the second variable has a larger effect on our outcome than the others is </span><span class="si">{</span><span class="n">p_indicator</span><span class="si">}</span><span class="s">"</span>
<span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The probability that the second variable has a larger effect on our outcome than the others is 0.0
</code></pre></div></div>

<p>We can compute realisations from the distributions indexed by our samples for each observation - the “posterior predictive distribution” - and compare our observations to the resulting distributions. This allows us to plot the histograms of the distributions and a tail p-value  for each observation . In the plots below I plot the difference between the predictions and the observation corresponding to them, so 0 is the true value. Very low or very high p-values are cause for concern, but this is a moot point here because the model corresponds perfectly to the data generating process.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">posterior_mu</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">samples</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">T</span>
<span class="n">posterior_sd</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">samples</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">post_preds</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">posterior_mu</span><span class="p">,</span> <span class="n">posterior_sd</span><span class="p">).</span><span class="n">rvs</span><span class="p">((</span><span class="mi">1000</span><span class="p">,</span> <span class="n">samples</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

<span class="n">post_pred_errors</span> <span class="o">=</span> <span class="n">post_preds</span> <span class="o">-</span> <span class="n">y</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">rand_obs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="mi">6</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">obs</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">rand_obs</span><span class="p">):</span>
    <span class="c1"># print(f"i {i} row {(i)//2 + 1} , col {(i) % 2 }")
</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">].</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">post_pred_errors</span><span class="p">[</span><span class="n">obs</span><span class="p">])</span>

    <span class="n">ax</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">].</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">"r"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span>
        <span class="sa">f</span><span class="s">"obs </span><span class="si">{</span><span class="n">obs</span><span class="si">}</span><span class="s">, tail p = </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">post_pred_errors</span><span class="p">[</span><span class="n">obs</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">).</span><span class="nb">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s">"</span>
    <span class="p">)</span>

<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">();</span>
</code></pre></div></div>

<p><img src="/assets/images/output_28_0.png" alt="png" /></p>

<h3 id="footnotes">Footnotes</h3>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>The density for the logit $y$ of the Beta variable is</p>

\[f(y|a,b) = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}l(y)^{a-1}(1-l(y))^{b-1}l(y)^2\exp(-y)\]

      <p>Where $l$ is the inverse logit function</p>

\[l(x) =  \frac{1}{1+\exp(-x)}\]

      <p>The distribution’s normalisation constant by definition doesn’t depend on $x$ and we can ignore it for the purposes of the algorithm, so we use the unnormalised log density for the Beta distribution</p>

\[\log(f(y \mid a, b)) = (a-1) \dot \log\left[l(y)\right] + (b-1) \dot \log\left[1-l(y)\right] - y - 2\log\left[1+\exp(-y)\right]\]

      <p>so $l(x) \in [0,1]$.</p>

      <p>We need the derivative for this function in order to use HMC. It is</p>

\[\frac{\partial f(y|a,b)}{\partial y} = \left(\frac{a-1}{l(y)} - \frac{b-1}{1-l(y)}\right) l(y)^2 \exp(-y) - 1 + 2l(y)\exp(-y)\]
      <p><a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>An alternative transform is to work with $\sigma$ and let the squaring take care of the positivity. However, the log likelihood contains a log step that is undefined for negative sigma proposals. Specifically $\log\left[\frac{1}{\sigma^N}\right] = -N\log(\sigma)$ <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>C Ludowici, PhD</name></author><category term="MCMC" /><summary type="html"><![CDATA[Hamiltonian Monte Carlo From Scratch in Python]]></summary></entry></feed>